---
title: "Lab 8: Machine Learning"
format: html
---




## Data Import/Tidy/Transform

**Library Loading**




```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(readr)
library(purrr)
```




**Data Ingest**




```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
local_files   <- glue('data/camels_{types}.txt')
camels <- map(local_files, read_delim, show_col_types = FALSE)

camels <- power_full_join(camels ,by = 'gauge_id')
```




**Data Cleaning**




```{r}
glimpse(camels)
skimr::skim(camels)
visdat::vis_dat(camels)

camels_clean <- camels %>% 
  drop_na()
```

```{r}
camels_clean %>%
  ggplot(aes(x = q_mean, y =  )) +
  geom_boxplot() +
  theme_linedraw()
```




## Data Splitting

**Initial Split & Testing/Training**




```{r}
set.seed(123)

camels <- camels |> 
  mutate(logQmean = log(q_mean))


camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

```




## Feature Engineering

**Proper Recipe**




```{r}
rec <- recipe(logQmean ~ p_seasonality + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ p_mean:p_seasonality) |> 
  step_naomit(all_predictors(), all_outcomes())

```

```{r}
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)
```




## Data Resampling and Model Testing

**Cross Validation Datasets**




```{r}
camels_cv <- vfold_cv(camels_train, v = 10)
```




**Define Three Regression Models**




```{r}
rf_model <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

xgb_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")
```




**Workflow Set/Map/Autoplot**




```{r}

wf <- workflow_set(list(rec), list(nn_model, rf_model, xgb_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```




**Model Selection with Justification** The model I am going to move forward with is the neural network because the visualized metrics indicate that it has the highest r-squared and lowest root mean squared error of the defined models. This model is a neural net engine and regression mode. I think it is performing well for this problem because

## Model Tuning

**Tunable model setup**




```{r}
nn_model_tuned = bag_mlp(epochs = tune(),
                         penalty = tune()) %>% 
  set_engine("nnet") %>%
  set_mode("regression")
  
```




**Tunable workflow**




```{r}
wf_tune <- workflow(rec, nn_model_tuned)
```




**Description of Dial Ranges**




```{r}
dials <- extract_parameter_set_dials(wf_tune)

dials$object
```




**Defined Search Space**




```{r}
my.grid <- dials %>% 
  grid_latin_hypercube(size = 20)
```




**Executed Tune Grid**




```{r}
model_params <-  tune_grid(
    wf_tune,
    resamples = camels_cv,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)
```




## Check Skill of Tuned Model

**Collect Metrics/Show Best/Describe in Plain Language**




```{r}
collect_metrics(model_params)

show_best(model_params, metric = "mae")
```

```{r}
hp_best <- select_best(model_params, metric = "mae")
```




## Finalize Model

**Finalize Workflow**




```{r}
wf_final <- finalize_workflow(wf_tune, hp_best)
```




## Final Model Verification

**Implement the last fit**




```{r}
final_fit <- last_fit(
  wf_final,
  split = camels_split
)
```




**Interpret Metrics**




```{r}
collect_metrics(final_fit)
preds <- collect_predictions(final_fit)
```




**Plot Predictions**




```{r}
library(ggplot2)

ggplot(preds, aes(x = .pred, y = logQmean)) +
  geom_point(aes(color = .pred), alpha = 0.6, size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "darkblue", linetype = "dashed") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dotted") +
  scale_color_viridis_c(option = "C") +
  labs(
    title = "Predicted vs Actual q_mean",
    x = "Predicted q_mean",
    y = "Actual q_mean",
    color = "Prediction"
  ) +
  theme_linedraw(base_size = 14)

```




## Final Figure

**Augment Data & Calculate Residuals**




```{r}
camels_data_cleaned <- camels_clean %>%
  mutate(logQmean = log(q_mean))
```

```{r}
final_fit_full <- fit(wf_final, data = camels_data_cleaned)

augmented_preds <- augment(final_fit_full, new_data = camels_data_cleaned)

augmented_preds <- augmented_preds %>% 
  mutate(residual = (.pred - logQmean)^2)
```




**Map Predicted Q and Residuals**




```{r}
map_pred <- ggplot(augmented_preds, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = .pred)) +
  scale_color_viridis_c(option = "C") +
  labs(title = "Predicted q_mean", color = "Prediction") +
  theme_linedraw() +
  coord_fixed()

map_resid <- ggplot(augmented_preds, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = residual)) +
  scale_color_viridis_c(option = "B") +
  labs(title = "Residuals (Squared)", color = "Residual") +
  theme_linedraw() +
  coord_fixed()

library(patchwork)
map <- map_pred + map_resid
print(map)
```

