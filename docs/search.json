[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab 6",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "index.html#question-1",
    "href": "index.html#question-1",
    "title": "Lab 6",
    "section": "Question 1",
    "text": "Question 1\nzero_q_freq represents the frequency of days with Q = 0 mm/day as a %.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "index.html#question-2",
    "href": "index.html#question-2",
    "title": "Lab 6",
    "section": "Question 2",
    "text": "Question 2\n\np1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  labs(title = \"Aridity of Sites\") +\n  ggthemes::theme_map()\n\np2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map() + \n  labs(title = \"P Mean of Sites\")\nlibrary(patchwork)\nprint(p1 | p2)\n\n\n\n\n\n\n\n\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nlibrary(baguette)\nlibrary(ranger)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2"
  },
  {
    "objectID": "index.html#question-3",
    "href": "index.html#question-3",
    "title": "Lab 6",
    "section": "Question 3",
    "text": "Question 3\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, xgb_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.547  0.0308    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.787  0.0266    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0247    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0258    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\nI would move forward with the neural network model because it has the highest r-squared value."
  },
  {
    "objectID": "index.html#question-4",
    "href": "index.html#question-4",
    "title": "Lab 6",
    "section": "Question 4",
    "text": "Question 4\nData Splitting\n\nset.seed(123)\n\ncamels1 &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels1, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nRecipe\n\nrec1 &lt;-  recipe(logQmean ~ aridity + p_mean + soil_porosity, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;%  \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec1, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base1 &lt;- lm(logQmean ~ ., data = baked_data)\nsummary(lm_base1)\n\n\nCall:\nlm(formula = logQmean ~ ., data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.93279 -0.21727  0.00367  0.21075  2.82522 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.49567    0.41223  -6.054 2.79e-09 ***\naridity          -0.94477    0.17282  -5.467 7.26e-08 ***\np_mean            1.41806    0.16511   8.588  &lt; 2e-16 ***\nsoil_porosity    -0.95656    0.50564  -1.892   0.0591 .  \naridity_x_p_mean  0.09854    0.07584   1.299   0.1945    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5729 on 497 degrees of freedom\nMultiple R-squared:  0.771, Adjusted R-squared:  0.7691 \nF-statistic: 418.3 on 4 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nI am adding soil porosity as a predictor in my model because I think it does have an impact on daily discharge.\nDefine 3 Models\n\nrf_mod &lt;- rand_forest() %&gt;% \n  set_engine('ranger') %&gt;% \n  set_mode(\"regression\")\n\nb_mod &lt;- boost_tree() %&gt;% \n  set_engine('xgboost') %&gt;% \n  set_mode(\"regression\")\n\nnn_mod &lt;- mlp(hidden = 10) %&gt;% \n  set_engine('nnet') %&gt;% \n  set_mode(\"regression\")\n\nWorkflow set\n\nwf1 &lt;- workflow_set(list(rec1), list(\n                                   rf_mod,\n                                   b_mod,\n                                   nn_mod)) %&gt;% \n  workflow_map(resamples = camels_cv)\n\nEvaluation I think the random forest model is the best because it has the highest r-squared and lowest root mean squared error.\n\nautoplot(wf1) \n\n\n\n\n\n\n\nranked_results &lt;- rank_results(wf1, rank_metric = \"rsq\",\n               select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.509  0.0270    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.819  0.0185    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.542  0.0300    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.801  0.0225    10 recipe       boos…     2\n5 recipe_mlp        Prepro… rmse    0.548  0.0321    10 recipe       mlp       3\n6 recipe_mlp        Prepro… rsq     0.790  0.0257    10 recipe       mlp       3\n\n\nExtract and Evaluate\n\nrf_wf1 &lt;- workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(rf_mod) %&gt;%\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf1, new_data = camels_test)\ndim(rf_data)\n\n[1] 168  60\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"purple\") +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nOverall, I think this model does a pretty good job of predicting mean daily discharge. The scatter of points around the line indicate that there are some prediction errors, but the general trend highlights the fact that the model is capturing the underlying relationship."
  }
]