[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab 6",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "index.html#question-1",
    "href": "index.html#question-1",
    "title": "Lab 6",
    "section": "Question 1",
    "text": "Question 1\nzero_q_freq represents the frequency of days with Q = 0 mm/day as a %.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "index.html#question-2",
    "href": "index.html#question-2",
    "title": "Lab 6",
    "section": "Question 2",
    "text": "Question 2\n\np1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  labs(title = \"Aridity of Sites\") +\n  ggthemes::theme_map()\n\np2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map() + \n  labs(title = \"P Mean of Sites\")\nlibrary(patchwork)\nprint(p1 | p2)\n\n\n\n\n\n\n\n\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nlibrary(baguette)\nlibrary(ranger)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.587\n2 rsq     standard       0.741\n3 mae     standard       0.363\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2"
  },
  {
    "objectID": "index.html#question-3",
    "href": "index.html#question-3",
    "title": "Lab 6",
    "section": "Question 3",
    "text": "Question 3\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, xgb_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.547  0.0308    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.787  0.0266    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0247    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0258    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\nI would move forward with the neural network model because it has the highest r-squared value."
  },
  {
    "objectID": "index.html#question-4",
    "href": "index.html#question-4",
    "title": "Lab 6",
    "section": "Question 4",
    "text": "Question 4\nData Splitting\n\nset.seed(123)\n\ncamels1 &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels1, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nRecipe\n\nrec1 &lt;-  recipe(logQmean ~ aridity + p_mean + soil_porosity, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;%  \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec1, camels_train) |&gt; \n  bake(new_data = NULL)\n\nlm_base1 &lt;- lm(logQmean ~ ., data = baked_data)\nsummary(lm_base1)\n\n\nCall:\nlm(formula = logQmean ~ ., data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.93279 -0.21727  0.00367  0.21075  2.82522 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.49567    0.41223  -6.054 2.79e-09 ***\naridity          -0.94477    0.17282  -5.467 7.26e-08 ***\np_mean            1.41806    0.16511   8.588  &lt; 2e-16 ***\nsoil_porosity    -0.95656    0.50564  -1.892   0.0591 .  \naridity_x_p_mean  0.09854    0.07584   1.299   0.1945    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5729 on 497 degrees of freedom\nMultiple R-squared:  0.771, Adjusted R-squared:  0.7691 \nF-statistic: 418.3 on 4 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nI am adding soil porosity as a predictor in my model because I think it does have an impact on daily discharge.\nDefine 3 Models\n\nrf_mod &lt;- rand_forest() %&gt;% \n  set_engine('ranger') %&gt;% \n  set_mode(\"regression\")\n\nb_mod &lt;- boost_tree() %&gt;% \n  set_engine('xgboost') %&gt;% \n  set_mode(\"regression\")\n\nnn_mod &lt;- mlp(hidden = 10) %&gt;% \n  set_engine('nnet') %&gt;% \n  set_mode(\"regression\")\n\nWorkflow set\n\nwf1 &lt;- workflow_set(list(rec1), list(\n                                   rf_mod,\n                                   b_mod,\n                                   nn_mod)) %&gt;% \n  workflow_map(resamples = camels_cv)\n\nEvaluation I think the random forest model is the best because it has the highest r-squared and lowest root mean squared error.\n\nautoplot(wf1) \n\n\n\n\n\n\n\nranked_results &lt;- rank_results(wf1, rank_metric = \"rsq\",\n               select_best = TRUE)\nprint(ranked_results)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.509  0.0270    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.819  0.0185    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.542  0.0300    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.801  0.0225    10 recipe       boos…     2\n5 recipe_mlp        Prepro… rmse    0.548  0.0321    10 recipe       mlp       3\n6 recipe_mlp        Prepro… rsq     0.790  0.0257    10 recipe       mlp       3\n\n\nExtract and Evaluate\n\nrf_wf1 &lt;- workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(rf_mod) %&gt;%\n  fit(data = camels_train) \n\n\nrf_data &lt;- augment(rf_wf1, new_data = camels_test)\ndim(rf_data)\n\n[1] 168  60\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_gradient(low = \"pink\", high = \"purple\") +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nOverall, I think this model does a pretty good job of predicting mean daily discharge. The scatter of points around the line indicate that there are some prediction errors, but the general trend highlights the fact that the model is capturing the underlying relationship."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Lab 8: Machine Learning",
    "section": "",
    "text": "Library Loading\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(readr)\nlibrary(purrr)\n\nData Ingest\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nlocal_files   &lt;- glue('data/camels_{types}.txt')\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nData Cleaning\n\nglimpse(camels)\n\nRows: 671\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"djf\", \"mam\", \"mam\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4489279, 0.308…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.17972945, 0.16461821, 0.28701001, 0.44386282, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.052140094, 0.02625797…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0747, 0.0522, 0.0711, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -14.8410, -14.4819,…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 45.17501, 44.86920,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -69.31470, -69.9…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 247.80, 310.38, 615.70, 47…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 29.56035, 49.92122,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 769.05, 909.10, 383.82,…\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 766.53, 904.94, 396.10,…\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9548, 0.9906, 1.0000, 1…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 4.903259, 5.086811,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 3.990843, 4.300978,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.8706685, 0.891…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.3986194, 0.445…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 1.0000000, 0.850…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2500000, 0.2410270, 0.225615…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.400000, 2.340180, 2.237435, 2…\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n\nskimr::skim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1.00\n8\n8\n0\n671\n0\n\n\nhigh_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1.00\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n138\n0.79\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1.00\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\n\n\nvisdat::vis_dat(camels)\n\n\n\n\n\n\n\ncamels_clean &lt;- camels %&gt;% \n  drop_na()\n\n\ncamels_clean %&gt;%\n  ggplot(aes(x = q_mean, y =  )) +\n  geom_boxplot() +\n  theme_linedraw()"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-importtidytransform",
    "href": "hyperparameter-tuning.html#data-importtidytransform",
    "title": "Lab 8: Machine Learning",
    "section": "",
    "text": "Library Loading\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(readr)\nlibrary(purrr)\n\nData Ingest\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nlocal_files   &lt;- glue('data/camels_{types}.txt')\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nData Cleaning\n\nglimpse(camels)\n\nRows: 671\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"djf\", \"mam\", \"mam\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4489279, 0.308…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.17972945, 0.16461821, 0.28701001, 0.44386282, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.052140094, 0.02625797…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0747, 0.0522, 0.0711, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -14.8410, -14.4819,…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 45.17501, 44.86920,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -69.31470, -69.9…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 247.80, 310.38, 615.70, 47…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 29.56035, 49.92122,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 769.05, 909.10, 383.82,…\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 766.53, 904.94, 396.10,…\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9548, 0.9906, 1.0000, 1…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 4.903259, 5.086811,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 3.990843, 4.300978,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.8706685, 0.891…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.3986194, 0.445…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 1.0000000, 0.850…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2500000, 0.2410270, 0.225615…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.400000, 2.340180, 2.237435, 2…\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n\nskimr::skim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1.00\n8\n8\n0\n671\n0\n\n\nhigh_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1.00\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n138\n0.79\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1.00\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\n\n\nvisdat::vis_dat(camels)\n\n\n\n\n\n\n\ncamels_clean &lt;- camels %&gt;% \n  drop_na()\n\n\ncamels_clean %&gt;%\n  ggplot(aes(x = q_mean, y =  )) +\n  geom_boxplot() +\n  theme_linedraw()"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-splitting",
    "href": "hyperparameter-tuning.html#data-splitting",
    "title": "Lab 8: Machine Learning",
    "section": "Data Splitting",
    "text": "Data Splitting\nInitial Split & Testing/Training\n\nset.seed(123)\n\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)"
  },
  {
    "objectID": "hyperparameter-tuning.html#feature-engineering",
    "href": "hyperparameter-tuning.html#feature-engineering",
    "title": "Lab 8: Machine Learning",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nProper Recipe\n\nrec &lt;- recipe(logQmean ~ p_seasonality + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ p_mean:p_seasonality) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\n\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\nWarning in bake.step_log(x$steps[[i]], new_data = training): NaNs produced"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-resampling-and-model-testing",
    "href": "hyperparameter-tuning.html#data-resampling-and-model-testing",
    "title": "Lab 8: Machine Learning",
    "section": "Data Resampling and Model Testing",
    "text": "Data Resampling and Model Testing\nCross Validation Datasets\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nDefine Three Regression Models\n\nrf_model &lt;- rand_forest() %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nWorkflow Set/Map/Autoplot\n\nwf &lt;- workflow_set(list(rec), list(nn_model, rf_model, xgb_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\n→ A | warning: NaNs produced\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\nThere were issues with some computations   A: x20\n\n\n\n\n\n→ A | warning: NaNs produced\n→ A | warning: NaNs produced\n\nautoplot(wf)\n\n\n\n\n\n\n\n\nModel Selection with Justification The model I am going to move forward with is the neural network because the visualized metrics indicate that it has the highest r-squared and lowest root mean squared error of the defined models. This model is a bagged multilayer percetron model that combines neural networks with bootstrap aggregation. The engine is nnet and the mode is regression. I think it is performing well for this problem because of its flexibility and ability to capture complex relationships."
  },
  {
    "objectID": "hyperparameter-tuning.html#model-tuning",
    "href": "hyperparameter-tuning.html#model-tuning",
    "title": "Lab 8: Machine Learning",
    "section": "Model Tuning",
    "text": "Model Tuning\nTunable model setup\n\nnn_model_tuned = bag_mlp(epochs = tune(),\n                         penalty = tune()) %&gt;% \n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nTunable workflow\n\nwf_tune &lt;- workflow(rec, nn_model_tuned)\n\nDescription of Dial Ranges\n\ndials &lt;- extract_parameter_set_dials(wf_tune)\n\ndials$object\n\n[[1]]\n\n\nAmount of Regularization (quantitative)\n\n\nTransformer: log-10 [1e-100, Inf]\n\n\nRange (transformed scale): [-10, 0]\n\n\n\n[[2]]\n\n\n# Epochs (quantitative)\n\n\nRange: [10, 1000]\n\n\nDefined Search Space\n\nmy.grid &lt;- dials %&gt;% \n  grid_latin_hypercube(size = 20)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\n\nExecuted Tune Grid\n\nmodel_params &lt;-  tune_grid(\n    wf_tune,\n    resamples = camels_cv,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\n\n→ A | warning: NaNs produced\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x13\n\n\nThere were issues with some computations   A: x19\n\n\nThere were issues with some computations   A: x25\n\n\nThere were issues with some computations   A: x31\n\n\nThere were issues with some computations   A: x38\n\n\nThere were issues with some computations   A: x42\n\n\nThere were issues with some computations   A: x48\n\n\nThere were issues with some computations   A: x55\n\n\nThere were issues with some computations   A: x61\n\n\nThere were issues with some computations   A: x67\n\n\nThere were issues with some computations   A: x73\n\n\nThere were issues with some computations   A: x80\n\n\nThere were issues with some computations   A: x86\n\n\nThere were issues with some computations   A: x91\n\n\nThere were issues with some computations   A: x98\n\n\nThere were issues with some computations   A: x103\n\n\nThere were issues with some computations   A: x109\n\n\nThere were issues with some computations   A: x115\n\n\nThere were issues with some computations   A: x122\n\n\nThere were issues with some computations   A: x128\n\n\nThere were issues with some computations   A: x132\n\n\nThere were issues with some computations   A: x139\n\n\nThere were issues with some computations   A: x145\n\n\nThere were issues with some computations   A: x151\n\n\nThere were issues with some computations   A: x157\n\n\nThere were issues with some computations   A: x164\n\n\nThere were issues with some computations   A: x169\n\n\nThere were issues with some computations   A: x174\n\n\nThere were issues with some computations   A: x182\n\n\nThere were issues with some computations   A: x187\n\n\nThere were issues with some computations   A: x193\n\n\nThere were issues with some computations   A: x199\n\n\nThere were issues with some computations   A: x206\n\n\nThere were issues with some computations   A: x210\n\n\n\n\nautoplot(model_params)"
  },
  {
    "objectID": "hyperparameter-tuning.html#check-skill-of-tuned-model",
    "href": "hyperparameter-tuning.html#check-skill-of-tuned-model",
    "title": "Lab 8: Machine Learning",
    "section": "Check Skill of Tuned Model",
    "text": "Check Skill of Tuned Model\nCollect Metrics/Show Best/Describe in Plain Language\n\ncollect_metrics(model_params)\n\n# A tibble: 60 × 8\n         penalty epochs .metric .estimator  mean     n std_err .config          \n           &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n 1 0.00000266       306 mae     standard   0.370    10  0.0286 Preprocessor1_Mo…\n 2 0.00000266       306 rmse    standard   0.642    10  0.0849 Preprocessor1_Mo…\n 3 0.00000266       306 rsq     standard   0.718    10  0.0410 Preprocessor1_Mo…\n 4 0.00000000938    900 mae     standard   0.422    10  0.0446 Preprocessor1_Mo…\n 5 0.00000000938    900 rmse    standard   0.746    10  0.126  Preprocessor1_Mo…\n 6 0.00000000938    900 rsq     standard   0.667    10  0.0593 Preprocessor1_Mo…\n 7 0.00000000117    739 mae     standard   0.387    10  0.0301 Preprocessor1_Mo…\n 8 0.00000000117    739 rmse    standard   0.704    10  0.0737 Preprocessor1_Mo…\n 9 0.00000000117    739 rsq     standard   0.672    10  0.0328 Preprocessor1_Mo…\n10 0.0000000256     415 mae     standard   0.345    10  0.0301 Preprocessor1_Mo…\n# ℹ 50 more rows\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n   penalty epochs .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000471    222 mae     standard   0.305    10  0.0224 Preprocessor1_Model07\n2 0.0182      583 mae     standard   0.310    10  0.0273 Preprocessor1_Model12\n3 0.00171     132 mae     standard   0.311    10  0.0240 Preprocessor1_Model15\n4 0.00580     189 mae     standard   0.323    10  0.0263 Preprocessor1_Model13\n5 0.0637      906 mae     standard   0.325    10  0.0242 Preprocessor1_Model10\n\n\nThis indicates that the best hyperparameter set is Penalty: 0.0004710049 and Epochs: 222.\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")"
  },
  {
    "objectID": "hyperparameter-tuning.html#finalize-model",
    "href": "hyperparameter-tuning.html#finalize-model",
    "title": "Lab 8: Machine Learning",
    "section": "Finalize Model",
    "text": "Finalize Model\nFinalize Workflow\n\nwf_final &lt;- finalize_workflow(wf_tune, hp_best)"
  },
  {
    "objectID": "hyperparameter-tuning.html#final-model-verification",
    "href": "hyperparameter-tuning.html#final-model-verification",
    "title": "Lab 8: Machine Learning",
    "section": "Final Model Verification",
    "text": "Final Model Verification\nImplement the last fit\n\nfinal_fit &lt;- last_fit(\n  wf_final,\n  split = camels_split\n)\n\n→ A | warning: NaNs produced\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\n\n\n\nInterpret Metrics\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.567 Preprocessor1_Model1\n2 rsq     standard       0.757 Preprocessor1_Model1\n\npreds &lt;- collect_predictions(final_fit)\n\nThese results indicate that the final model is performing well on the test data. The rmse of the test data model is slightly higher than the training model but the r-squared is also slightly higher.\nPlot Predictions\n\nlibrary(ggplot2)\n\nggplot(preds, aes(x = .pred, y = logQmean)) +\n  geom_point(aes(color = .pred), alpha = 0.6, size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\", linetype = \"dashed\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dotted\") +\n  scale_color_viridis_c(option = \"C\") +\n  labs(\n    title = \"Predicted vs Actual q_mean\",\n    x = \"Predicted q_mean\",\n    y = \"Actual q_mean\",\n    color = \"Prediction\"\n  ) +\n  theme_linedraw(base_size = 14)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 50 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 50 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "hyperparameter-tuning.html#final-figure",
    "href": "hyperparameter-tuning.html#final-figure",
    "title": "Lab 8: Machine Learning",
    "section": "Final Figure",
    "text": "Final Figure\nAugment Data & Calculate Residuals\n\ncamels_data_cleaned &lt;- camels_clean %&gt;%\n  mutate(logQmean = log(q_mean))\n\n\nfinal_fit_full &lt;- fit(wf_final, data = camels_data_cleaned)\n\nWarning in bake.step_log(x$steps[[i]], new_data = training): NaNs produced\n\naugmented_preds &lt;- augment(final_fit_full, new_data = camels_data_cleaned)\n\nWarning in bake.step_log(step, new_data = new_data): NaNs produced\n\naugmented_preds &lt;- augmented_preds %&gt;% \n  mutate(residual = (.pred - logQmean)^2)\n\nMap Predicted Q and Residuals\n\nmap_pred &lt;- ggplot(augmented_preds, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = .pred)) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Predicted q_mean\", color = \"Prediction\") +\n  theme_linedraw() +\n  coord_fixed()\n\nmap_resid &lt;- ggplot(augmented_preds, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = residual)) +\n  scale_color_viridis_c(option = \"B\") +\n  labs(title = \"Residuals (Squared)\", color = \"Residual\") +\n  theme_linedraw() +\n  coord_fixed()\n\nlibrary(patchwork)\nmap &lt;- map_pred + map_resid\nprint(map)"
  }
]